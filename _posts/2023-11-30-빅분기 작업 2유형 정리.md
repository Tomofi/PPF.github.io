---
title : 빅분기 데이터 작업 2유형 정리
date : 2023-11-30 16:35:00 +/09:00
categories : [Devlog, DevTip]
tags : [DevTip] #소문자만 가능
---



----

### 메모

#### (1) 분류

##### 1. 데이터 전처리
###### 1-1. 데이터 탐색

> * x_train.info(): 기본 정보 확인
> * x_train.nunique(): 각 컬럼 별 유일값 갯수
> * x_train.isnull().sum(): 각 컬럼 별 결측값 갯수

&nbsp;
###### 1-2. 데이터 클리닝
> **데이터 컬럼 제거**
> * drop_col = \['컬럼']: 제거 할 컬럼 저장
> * x_train_drop = x_train.drop(columns = drop_col): 훈련 데이터 컬럼 제거
> * x_test_drop = x_test.drop(columns = drop_col): 평가 데이터 컬럼 제거

&nbsp;

> **(결측치 제거 및 대체)**
> * (1) 결측치 모두 통으로 제거: 추천X
> * (2) 평균값 or 중앙값으로 대체
> 	- fillna(df.컬럼.mean())
> 	- fillna(df.컬럼.median())


&nbsp;
###### 1-3. 데이터셋 인코딩 / 스케일링 / 분할
> **데이터셋 인코딩**
> **- 원핫인코딩: 더미화**
> * x_train_dum = pd.get_dummies(x_train_drop): 훈련 데이터 더미화
> * x_test_dum = pd.get_dummies(x_test_drop): 평가 데이터 더미화
> * x_test_dum = x_test_dum\[x_train_dum.columns]: 훈련 데이터와 컬럼 순서 동일하게
>
> **(라벨인코딩)**
> * from sklearn.preprocessing import LabelEncoder
> * encoder = LabelEncoder()
> * label = ['컬럼1', '컬럼2']
> * df\[label] = df\[label].apply(encoder.fit_transform): 한 번에 라벨 인코딩
> * encoder.fit_transform(df.컬럼1): 컬럼1 열에 대해서만 라벨 인코딩

&nbsp;

> **(파생변수 생성(구간화(비닝))**
> - 수치형(연속형)값들을 이산형 또는 범주형으로 변환
> * df\['컬럼1_qcut'] = pd.qcut(df\['컬럼1'], 5, labels=False): 5구간으로 나눔
> * df\['컬럼1_qcut'].value_counts(): 5구간으로 잘 나누어 졌는지 확인

&nbsp;

> **(스케일링)**
> * from sklearn.preprocessing import MinMaxScaler
> * scale_list = \['컬럼1','컬럼2']
> * scaler = MinMaxScaler()
> 	- 동시에
> 		- df\[scale_list] = sclaer.fit_transform(df\[scale_list]) : fit, transform 동시에
> 	- 따로
> 		- df\[scale_list] = sclaer.fit(df\[scale_list]) 
> 		- df\[scale_list] = sclaer.transform(df\[scale_list])
 
&nbsp;

> **y 라벨 값 할당**
> * y = y_train['target'].astype('int'): 타입 변환 필요시에는 astype()

&nbsp;

> **데이터셋 분할**
> * from sklearn.model_selection import train_test_split
> * xtr, xt, ytr, yt = train_test_split(x_train_dum, y, test_size=0.33, random_state = 42)

&nbsp;

##### 2. 모델 학습 / 평가 / 튜닝
###### 2-1. 모델 학습
> * from sklearn.ensemble import RandomForestClassifier
> * rf = RandomForestClassifier(random_state=23): 모델 생성
> * rf.fit(xtr,ytr): 모델 학습

&nbsp;
###### 2-2. 모델 평가
> * from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score

> **학습된 모델 활용한 예측**
> * x_train_pred = rf.predict(xtr): 훈련 데이터 분류 예측 
> * x_valid_pred = rf.predict(xt): 검증 데이터 분류 예측 
> * x_test_pred = rf.predict(x_test_dum): 평가 데이터 분류 예측 (제출)

> * x_train_prob = rf.predict_proba(xtr)\[:,1]: 훈련 데이터 분류 예측 확률
> * x_valid_prob = rf.predict_proba(xtr)\[:,1]: 검증 데이터 분류 예측 확률
> * x_test_prob = rf.predict_proba(x_test_dum)\[:,1]: 평가 데이터 분류 예측 확률 (제출)

&nbsp;


> **모델 평가**
> * accuracy_score(ytr, x_train_pred) : 훈련 데이터 accuracy 평가
> * accuracy_score(yt, x_valid_pred): 검증 데이터 accuracy 평가
> 
> * f1_score(ytr, x_train_pred) : 훈련 데이터 f1_score 평가
> * f1_score(yt, x_valid_pred): 검증 데이터 f1_score 평가
> 
> * recall_score(ytr, x_train_pred) : 훈련 데이터 recall 평가
> * recall_score(yt, x_valid_pred): 검증 데이터 recall 평가
> 
> * precision_score(ytr, x_train_pred) : 훈련 데이터 precision 평가
> * precision_score(yt, x_valid_pred): 검증 데이터 precision 평가
>
> * roc_auc_score(ytr, x_train_prob) : 훈련 데이터 roc_auc_score 평가
> * roc_auc_score(yt, x_valid_prob): 검증 데이터 roc_auc_score 평가

&nbsp;

###### 2-3. 모델 튜닝 
> **(하이퍼 파라미터 튜닝)**
> * from sklearn.model_selection import GridSearchCV
> * params = {'n_estimators' : [50, 100], 'max_depth' : [4, 6]} 
> * model5 = RandomForestClassifier()
> * clf = GridSearchCV(estimator = model5, param_grid = params , cv = 3)
> * clf.fit(x_train, y_train)
> * print ('최적의 하이퍼 파라미터는?', clf.best_params_)

&nbsp;

##### 3. 제출

###### 3-1. 제출
> **accuracy, f1_score, recall, precision**
> * pd.DataFrame({'CustomerId': x_test.CustomerId, 'Exited': x_test_pred}).to_csv('result.csv', index=False)

&nbsp;

> **auc, 확률**
> * pd.DataFrame({'CustomerId': x_test.CustomerId, 'Exited': x_test_prob}).to_csv('result.csv', index=False)

&nbsp;

> **index=False 필수!**

&nbsp;
&nbsp;

#### (2) 회귀

##### 1. 데이터 전처리

###### 1-1. 데이터 탐색
> * x_train.info(): 기본 정보 확인
> * x_train.nunique(): 각 컬럼 별 유일값 갯수
> * x_train.isnull().sum(): 각 컬럼 별 결측값 갯수

&nbsp;
###### 1-2. 데이터 클리닝
> **데이터 컬럼 제거**
> * drop_col = \['컬럼']: 제거 할 컬럼 저장
> * x_train_drop = x_train.drop(columns = drop_col): 훈련 데이터 컬럼 제거
> * x_test_drop = x_test.drop(columns = drop_col): 평가 데이터 컬럼 제거

&nbsp;

> **(결측치 제거 및 대체)**
> * (1) 결측치 모두 통으로 제거: 추천X
> * (2) 평균값 or 중앙값으로 대체
> 	- fillna(df.컬럼.mean())
> 	- fillna(df.컬럼.median())

&nbsp;
###### 1-3. 데이터셋 인코딩 / 스케일링/ 분할
> **데이터셋 인코딩**
> **- 원핫인코딩: get_dummies**
> * x_train_dum = pd.get_dummies(x_train_drop)
> * x_test_dum =  pd.get_dummies(x_test_drop)\[x_train_dum.columns]

> **(라벨 인코딩)**
> * from sklearn.preprocessing import LabelEncoder
> * encoder = LabelEncoder()
> * (1) df['컬럼1'] = df['컬럼1'].apply(encoder.fit_transform)
> * (2) encoder.fit_transform(df.컬럼1)

&nbsp;

> **(파생변수 생성(구간화(비닝))**
> - 수치형(연속형)값들을 이산형 또는 범주형으로 변환
> * df\['컬럼1_qcut'] = pd.qcut(df\['컬럼1'], 5, labels=False): 5구간으로 나눔
> * df\['컬럼1_qcut'].value_counts(): 5구간으로 잘 나누어 졌는지 확인

&nbsp;

> **(스케일링)**
> * from sklearn.preprocessing import MinMaxScaler
> * scale_list = ['컬럼1', '컬럼2']
> * scaler = MinMaxScaler()
> 	- 동시에
> 		* df\[scale_list] = scalar.fit_trainsform(df\[scale_list])
> 	- 따로
> 		* df\[scale_list] = scalar.fit(df\[scale_list])
> 		* df\[scale_list] = scaler.transform(df\[scale_list])

&nbsp;

> **y 라벨 값 할당**
> * y = y_train['target'].astype('int'): 타입 변환 필요시에는 astype()

&nbsp;

> **데이터셋 분할**
> * from sklearn.model_selection import train_test_split
> * xtr, xt, ytr, yt = train_test_split(x_train_dum, y)

&nbsp;

##### 2. 모델 학습 / 평가/ 튜닝

###### 2-1. 모델 학습
> * from sklearn.ensemble import RandomForestRegressor
> * rf = RandomForestRegressor(random_state=23)
> * rf.fit(xtr,ytr)

&nbsp;
###### 2-2. 모델 평가
> * from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
> * import numpy as np

> **학습된 모델을 활용한 예측**
> * x_train_pred = rf.predict(xtr)
> * x_valid_pred = rf.predict(xt)
> * x_test_pred = rf.predict(x_test_dum): 제출용

> **모델 평가**
> * mean_squared_error(ytr, x_train_pred)
> * mean_squared_error(yt, x_valid_pred)
> 
> * mean_absolute_error(ytr, x_train_pred)
> * mean_absolute_error(yt, x_valid_pred)
> 
> * mean_absolute_percentage_error(ytr, x_train_pred)
> * mean_absolute_percentage_error(yt, x_valid_pred)
> 
> * np.sqrt(mean_squared_error(ytr, x_train_pred))
> * np.sqrt(mean_squared_error(yt, x_valid_pred))
> 
> * r2_score(ytr, x_train_pred)
> * r2_score(yt, x_valid_pred)


&nbsp;
###### 2-3. 모델 튜닝
> **(하이퍼 파라미터 튜닝)**
> * from sklearn.model_selection import GridSearchCV
> * params = {'n_estimators' : [50, 100], 'max_depth' : [4, 6]} 
> * model5 = RandomForestClassifier()
> * clf = GridSearchCV(estimator = model5, param_grid = params , cv = 3)
> * clf.fit(x_train, y_train)
> * print ('최적의 하이퍼 파라미터는?', clf.best_params_)

&nbsp;
##### 3. 제출

###### 3-1. 제출
 **mse, mae, mape, rmse, r2_score**
> * pd.DataFrame({'CustomerId': x_test.CustomerId, 'Exited': x_test_pred}).to_csv('result.csv', index=False)

&nbsp;


### 출처(참고문헌)
* [작업 2유형 분류 Tip](https://aitzone.tistory.com/29)
* [작업 2유형 회귀 Tip](https://aitzone.tistory.com/30)


