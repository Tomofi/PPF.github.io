---
title : 빅분기 데이터 작업 2유형 정리(회귀)
date : 2023-11-30 16:35:00 +/09:00
categories : [Devlog, DevTip]
tags : [DevTip] #소문자만 가능
---



----

### 메모

#### (1) 회귀

##### 1. 데이터 전처리

###### 1-1. 데이터 탐색
```python
x_train.info()  # 기본 정보 확인
x_train.nunique() # 각 컬럼 별 유일값 갯수
x_train.isnull().sum() # 각 컬럼 별 결측값 갯수
```


&nbsp;
###### 1-2. 데이터 클리닝
> **데이터 컬럼 제거**
```python
drop_col = ['컬럼'] # 제거 할 컬럼 저장
x_train_drop = x_train.drop(columns = drop_col) # 훈련 데이터 컬럼 제거
x_test_drop = x_test.drop(columns = drop_col) # 평가 데이터 컬럼 제거
```

&nbsp;
> **(결측치 제거 및 대체)**
> (1) 결측치 모두 통으로 제거: 추천X
> (2) 평균값 or 중앙값으로 대체
```python
fillna(df.컬럼.mean())
fillna(df.컬럼.median())
```


&nbsp;
###### 1-3. 데이터셋 인코딩 / 스케일링 / 분할
> **데이터셋 인코딩**
> **원핫인코딩: 더미화**
```python
x_train_dum = pd.get_dummies(x_train_drop) # 훈련 데이터 더미화
x_test_dum = pd.get_dummies(x_test_drop) # 평가 데이터 더미화
x_test_dum = x_test_dum[x_train_dum.columns] # 훈련 데이터와 컬럼 순서 동일하게
```

&nbsp;
> **(라벨인코딩)**
```python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
label = ['컬럼1', '컬럼2']
df[label] = df[label].apply(encoder.fit_transform) # 한 번에 라벨 인코딩
encoder.fit_transform(df.컬럼1) # 컬럼1 열에 대해서만 라벨 인코딩
```

&nbsp;
> **(파생변수 생성(구간화(비닝))**
> - 수치형(연속형)값들을 이산형 또는 범주형으로 변환
```python
> df['컬럼1_qcut'] = pd.qcut(df\['컬럼1'], 5, labels=False) # 5구간으로 나눔
> df['컬럼1_qcut'].value_counts() # 5구간으로 잘 나누어 졌는지 확인
```

&nbsp;
> **(스케일링)**
```python
from sklearn.preprocessing import MinMaxScaler
scale_list = ['컬럼1','컬럼2']
scaler = MinMaxScaler()
# 동시에
df[scale_list] = sclaer.fit_transform(df[scale_list]) : fit, transform 동시에
# 따로
df[scale_list] = sclaer.fit(df[scale_list]) 
df[scale_list] = sclaer.transform(df[scale_list])
```

&nbsp; 
> **y 라벨 값 할당**
```python
y = y_train['target'].astype('int') # 타입 변환 필요시에는 astype()
```

&nbsp;
> **데이터셋 분할**
```python
from sklearn.model_selection import train_test_split
> xtr, xt, ytr, yt = train_test_split(x_train_dum, y, test_size=0.33, random_state = 42)
```

&nbsp;
##### 2. 모델 학습 / 평가/ 튜닝

###### 2-1. 모델 학습 
```python
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state=23)
rf.fit(xtr,ytr)
```

&nbsp;
###### 2-2. 모델 평가
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
import numpy as np
```

&nbsp;
> **학습된 모델을 활용한 예측**

```python
x_train_pred = rf.predict(xtr)
x_valid_pred = rf.predict(xt)
x_test_pred = rf.predict(x_test_dum): 제출용
```

&nbsp;
> **모델 평가**

```python
mean_squared_error(ytr, x_train_pred)
mean_squared_error(yt, x_valid_pred)
 
mean_absolute_error(ytr, x_train_pred)
mean_absolute_error(yt, x_valid_pred)
 
mean_absolute_percentage_error(ytr, x_train_pred)
mean_absolute_percentage_error(yt, x_valid_pred)
 
np.sqrt(mean_squared_error(ytr, x_train_pred))
np.sqrt(mean_squared_error(yt, x_valid_pred))
 
r2_score(ytr, x_train_pred)
r2_score(yt, x_valid_pred)
```


&nbsp;
###### 2-3. 모델 튜닝
> **(하이퍼 파라미터 튜닝)**
```python
from sklearn.model_selection import GridSearchCV
params = {'n_estimators' : [50, 100], 'max_depth' : [4, 6]} 
model5 = RandomForestClassifier()
clf = GridSearchCV(estimator = model5, param_grid = params , cv = 3)
clf.fit(x_train, y_train)
print ('최적의 하이퍼 파라미터는?', clf.best_params_)
```


&nbsp;
##### 3. 제출

###### 3-1. 제출
>  **mse, mae, mape, rmse, r2_score**
```python
pd.DataFrame({'CustomerId': x_test.CustomerId, 'Exited': x_test_pred}).to_csv('result.csv', index=False)
```

&nbsp;



### 출처(참고문헌)
* [작업 2유형 회귀 Tip](https://aitzone.tistory.com/30)


