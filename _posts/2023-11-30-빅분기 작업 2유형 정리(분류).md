---
title : 빅분기 데이터 작업 2유형 정리(분류)
date : 2023-11-30 16:35:00 +/09:00
categories : [Devlog, DevTip]
tags : [DevTip] #소문자만 가능
---



----
#### (1) 분류

##### 1. 데이터 전처리

###### 1-1. 데이터 탐색
```python
x_train.info()  # 기본 정보 확인
x_train.nunique() # 각 컬럼 별 유일값 갯수
x_train.isnull().sum() # 각 컬럼 별 결측값 갯수
```


&nbsp;
###### 1-2. 데이터 클리닝
> **데이터 컬럼 제거**
```python
drop_col = ['컬럼'] # 제거 할 컬럼 저장
x_train_drop = x_train.drop(columns = drop_col) # 훈련 데이터 컬럼 제거
x_test_drop = x_test.drop(columns = drop_col) # 평가 데이터 컬럼 제거
```

&nbsp;
> **(결측치 제거 및 대체)**
> (1) 결측치 모두 통으로 제거: 추천X
> (2) 평균값 or 중앙값으로 대체
```python
fillna(df.컬럼.mean())
fillna(df.컬럼.median())
```


&nbsp;
###### 1-3. 데이터셋 인코딩 / 스케일링 / 분할
> **데이터셋 인코딩**
> **원핫인코딩: 더미화**
```python
x_train_dum = pd.get_dummies(x_train_drop) # 훈련 데이터 더미화
x_test_dum = pd.get_dummies(x_test_drop) # 평가 데이터 더미화
x_test_dum = x_test_dum[x_train_dum.columns] # 훈련 데이터와 컬럼 순서 동일하게
```

&nbsp;
> **(라벨인코딩)**
```python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
label = ['컬럼1', '컬럼2']
df[label] = df[label].apply(encoder.fit_transform) # 한 번에 라벨 인코딩
encoder.fit_transform(df.컬럼1) # 컬럼1 열에 대해서만 라벨 인코딩
```

&nbsp;
> **(파생변수 생성(구간화(비닝))**
> - 수치형(연속형)값들을 이산형 또는 범주형으로 변환
```python
> df['컬럼1_qcut'] = pd.qcut(df\['컬럼1'], 5, labels=False) # 5구간으로 나눔
> df['컬럼1_qcut'].value_counts() # 5구간으로 잘 나누어 졌는지 확인
```

&nbsp;
> **(스케일링)**
```python
from sklearn.preprocessing import MinMaxScaler
scale_list = ['컬럼1','컬럼2']
scaler = MinMaxScaler()
# 동시에
df[scale_list] = sclaer.fit_transform(df[scale_list]) : fit, transform 동시에
# 따로
df[scale_list] = sclaer.fit(df[scale_list]) 
df[scale_list] = sclaer.transform(df[scale_list])
```

&nbsp; 
> **y 라벨 값 할당**
```python
y = y_train['target'].astype('int') # 타입 변환 필요시에는 astype()
```

&nbsp;
> **데이터셋 분할**
```python
from sklearn.model_selection import train_test_split
> xtr, xt, ytr, yt = train_test_split(x_train_dum, y, test_size=0.33, random_state = 42)
```

&nbsp;
##### 2. 모델 학습 / 평가 / 튜닝
###### 2-1. 모델 학습
> ```python
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=23) # 모델 생성
rf.fit(xtr,ytr) # 모델 학습
```



&nbsp;
###### 2-2. 모델 평가
> ```python
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score
```

&nbsp;
> **학습된 모델 활용한 예측**

```python
x_train_pred = rf.predict(xtr) # 훈련 데이터 분류 예측 
x_valid_pred = rf.predict(xt) # 검증 데이터 분류 예측 
x_test_pred = rf.predict(x_test_dum) # 평가 데이터 분류 예측 (제출)

x_train_prob = rf.predict_proba(xtr)[:,1] # 훈련 데이터 분류 예측 확률
x_valid_prob = rf.predict_proba(xtr)[:,1] # 검증 데이터 분류 예측 확률
x_test_prob = rf.predict_proba(x_test_dum)[:,1] # 평가 데이터 분류 예측 확률 (제출)
```

&nbsp;
> **모델 평가**

```python
accuracy_score(ytr, x_train_pred) # 훈련 데이터 accuracy 평가
accuracy_score(yt, x_valid_pred) #  검증 데이터 accuracy 평가
 
f1_score(ytr, x_train_pred) # 훈련 데이터 f1_score 평가
f1_score(yt, x_valid_pred) # 검증 데이터 f1_score 평가
 
recall_score(ytr, x_train_pred) # 훈련 데이터 recall 평가
recall_score(yt, x_valid_pred) # 검증 데이터 recall 평가
 
precision_score(ytr, x_train_pred) # 훈련 데이터 precision 평가
precision_score(yt, x_valid_pred) # 검증 데이터 precision 평가

roc_auc_score(ytr, x_train_prob) # 훈련 데이터 roc_auc_score 평가
roc_auc_score(yt, x_valid_prob) # 검증 데이터 roc_auc_score 평가
```

&nbsp;
###### 2-3. 모델 튜닝 
> **(하이퍼 파라미터 튜닝)**
```python
from sklearn.model_selection import GridSearchCV
params = {'n_estimators' : [50, 100], 'max_depth' : [4, 6]} 
model5 = RandomForestClassifier()
clf = GridSearchCV(estimator = model5, param_grid = params , cv = 3)
clf.fit(x_train, y_train)
print ('최적의 하이퍼 파라미터는?', clf.best_params_)
```

&nbsp;

##### 3. 제출

###### 3-1. 제출
> **accuracy, f1_score, recall, precision**
```python 
pd.DataFrame({'CustomerId': x_test.CustomerId, 'Exited': x_test_pred}).to_csv('result.csv', index=False)
```

&nbsp;
> **auc, 확률**
```python
pd.DataFrame({'CustomerId': x_test.CustomerId, 'Exited': x_test_prob}).to_csv('result.csv', index=False)
```

&nbsp;
> **index=False 필수!**

&nbsp;

### 출처(참고문헌)
* [작업 2유형 분류 Tip](https://aitzone.tistory.com/29)


